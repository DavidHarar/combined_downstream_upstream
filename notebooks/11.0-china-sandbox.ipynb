{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upstream Model File Path: ./upstream_seq2seq/models/transformer_cnn_4heads_1701990699.9536803.pt\n",
      "Num attention heads: 8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TSTransformerEncoderCNN(\n",
       "  (project_input): Linear(in_features=12, out_features=128, bias=True)\n",
       "  (encoder): Linear(in_features=12, out_features=128, bias=True)\n",
       "  (transformer_encoder): TransformerEncoder(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x TransformerEncoderLayer(\n",
       "        (self_attn): MultiheadAttention(\n",
       "          (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
       "        (dropout): Dropout(p=0.4, inplace=False)\n",
       "        (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (dropout1): Dropout(p=0.4, inplace=False)\n",
       "        (dropout2): Dropout(p=0.4, inplace=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (output_layer): Linear(in_features=128, out_features=12, bias=True)\n",
       "  (dropout1): Dropout(p=0.4, inplace=False)\n",
       "  (cnn): Conv1d(128, 12, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import argparse\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "import pickle \n",
    "\n",
    "# import trainer\n",
    "# -----------------------------\n",
    "# PC\n",
    "# sys.path.append('/home/david/Desktop/projects/thesis/downstream_classification')\n",
    "# os.chdir('/home/david/Desktop/projects/thesis/downstream_classification')\n",
    "\n",
    "# MAC\n",
    "sys.path.append('/Users/davidharar/Documents/School/thesis/git_repositories/')\n",
    "os.chdir('/Users/davidharar/Documents/School/thesis/git_repositories/')\n",
    "\n",
    "from combined_downstream_upstream.utils.LoadModels import *\n",
    "from combined_downstream_upstream.executors.train_combined_model import trainer\n",
    "\n",
    "\n",
    "# upstream model params and config\n",
    "# ---------------\n",
    "upstream_params = {\n",
    "    'input_dimension': 12,\n",
    "    'output_dimension': 12,              \n",
    "    'hidden_dimmension':  128,           # d_model (int) â€“ the number of expected features in the input (required)???,\n",
    "    'attention_heads': 8,               # number of attention heads, if None then d_model//64,\n",
    "    'encoder_number_of_layers': 8,\n",
    "    'dropout': 0.4,\n",
    "    'clip': 1,\n",
    "    'positional_encodings': False,\n",
    "    'device':'mps'\n",
    "}\n",
    "\n",
    "# run script\n",
    "# -----------------------------\n",
    "best_rocauc_and_pr_auc = {\n",
    "    'channels_to_turn_off':[],\n",
    "    'seed':[],\n",
    "    'validation-roc-auc':[],\n",
    "    'validation-pr-auc':[],\n",
    "    'test-roc-auc':[],\n",
    "    'test-pr-auc':[],\n",
    "}\n",
    "\n",
    "seed = 123\n",
    "np.random.seed(123)\n",
    "seeds = np.random.randint(0,1000, 15)\n",
    "\n",
    "config = {\n",
    "            # general\n",
    "            'targets': ['AF'],\n",
    "            'leads': ['LI', 'LII', 'LIII', 'aVL', 'aVR','aVF', 'V1','V2','V3','V4','V5','V6'],\n",
    "            # 'leads': ['LI', 'LII', 'LIII', 'aVF', 'aVL', 'aVR','V1','V2','V3','V4','V5','V6'],\n",
    "\n",
    "            # training\n",
    "            'batch_size': 128,\n",
    "            'n_epochs': 10, \n",
    "            'weight_decay': 0.3,\n",
    "            'lr': 0.0005,\n",
    "            'eval_metric':'aucpr',\n",
    "            'patience':3,\n",
    "            'clip':1,\n",
    "            'loss_function_weight':None,\n",
    "            'predefined_device':'mps',\n",
    "\n",
    "            # Experiment settings\n",
    "            'impute_only_missing':True,\n",
    "            'continue_training_upstream_model':False,\n",
    "\n",
    "            # test\n",
    "            'check_on_test':True,\n",
    "            'plot':False, # convert to true if want to plot\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "seed_ = 42\n",
    "\n",
    "# Empty leads\n",
    "# -------------------\n",
    "config['seed']=seed_\n",
    "config['metadata_file_path']    = './downstream_classification/data/combined_data/china.csv'\n",
    "config['data_folder_path']      = './downstream_classification/data/china-processed_divided_into_450/'\n",
    "\n",
    "config['internal_data']         = False\n",
    "config['channels_to_turn_off']  = 0\n",
    "config['model_saving_path']     = f\"./combined_downstream_upstream/models/china-{config['channels_to_turn_off']}_channels_off-no_additional_noise-impute_only_missing\"\n",
    "config['plot_saving_path']      = f\"/home/david/Desktop/projects/thesis/combined_downstream_upstream/plots/china-{config['channels_to_turn_off']}_channels_off-no_additional_noise-impute_only_missing\"\n",
    "\n",
    "upstream_model   = load_upstream_model(upstream_params, folder_path = './upstream_seq2seq/models/', model_name = 'transformer_cnn_4heads', device = upstream_params['device'])\n",
    "upstream_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
